# Olympiad-Style AI Assessment: Biology

This repository contains a structured assessment designed to evaluate the reasoning and problem-solving abilities of a large language model (o4-mini-high, accessed via ChatGPT Plus) on advanced, Olympiad-style questions rooted in biology and neuroscience.

## Overview

The assessment focuses on three original, high-difficulty questions inspired by real challenges in systems neuroscience, functional connectivity, and receptive field analysis. Each question was crafted to test the modelâ€™s reasoning depth and edge-case handling across quantitative and conceptual domains.

### Components

- **Original Questions**: Three challenging, domain-specific problems.
- **Model Responses**: Full outputs generated by the AI model.
- **Error Analyses**: Detailed breakdowns of where and why the model's reasoning or outputs were incorrect or incomplete.
- **Correct Solutions**: Step-by-step walkthroughs of the expected reasoning and correct answers.
- **Retrospective**: A critical reflection on the model's performance, including strengths, weaknesses, and insights into how it handles structured scientific reasoning.

## File

- [`assessment.md`](./assessment.md): Full write-up of the questions, model responses, analyses, and retrospective.
